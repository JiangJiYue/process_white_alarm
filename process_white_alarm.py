from pathlib import Path

import pandas as pd
import re
import logging
import os
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
import yaml
from logging import LoggerAdapter
from datetime import datetime

# å¯¼å…¥æŠ½ç¦»çš„ Ollama æ¨¡å—
from ollama_client import OllamaClient, create_ollama_client_from_config, test_ollama_connection

# ================== ä»é…ç½®æ–‡ä»¶åŠ è½½ ==================
with open("config.yaml", "r", encoding="utf-8") as f:
    config = yaml.safe_load(f)

# è·å–è¾“å‡ºç›®å½•é…ç½®ï¼Œå¦‚æœæ²¡æœ‰è®¾ç½®åˆ™ä½¿ç”¨é»˜è®¤å€¼
OUTPUT_DIR = config.get("output_dir", "output/run_{}".format(datetime.now().strftime("%Y%m%d_%H%M%S")))

# åœ¨è¾“å‡ºç›®å½•ä¸­å®šä¹‰è¾“å‡ºæ–‡ä»¶
INVALID_OUTPUT_FILE = os.path.join(OUTPUT_DIR, "invalid_records.xlsx")
RESULT_OUTPUT_FILE = os.path.join(OUTPUT_DIR, "valid_results.xlsx")

# æ—¥å¿—æ–‡ä»¶é…ç½®
LOG_DIR = config["logging"].get("log_dir", "logs")
LOG_FILE_TEMPLATE = config["logging"]["log_file"]

# åˆ›å»ºæ—¥å¿—ç›®å½•
os.makedirs(LOG_DIR, exist_ok=True)

# æ›¿æ¢å ä½ç¬¦ç”Ÿæˆæ—¥å¿—æ–‡ä»¶è·¯å¾„
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
LOG_FILE = LOG_FILE_TEMPLATE.replace("{log_dir}", LOG_DIR).replace("{timestamp}", timestamp).replace("{task_id}", "standalone")

OLLAMA_CONFIG = config["ollama"]

MAX_WORKERS = config["processing"]["max_workers"]
MAX_ROWS_TO_PROCESS = config["processing"]["max_rows_to_process"]  # å¯ä¸º null â†’ None

LOG_LEVEL = getattr(logging, config["logging"]["level"].upper())
LOG_FORMAT = config["logging"].get("format", "text")  # é»˜è®¤ä¸ºæ–‡æœ¬æ ¼å¼

SYSTEM_PROMPT = config["system_prompt"].rstrip()

# ===================================================

# ================== æ—¥å¿—é…ç½® ==================
# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs(OUTPUT_DIR, exist_ok=True)

# é…ç½®æ—¥å¿—æ ¼å¼
if LOG_FORMAT == "json":
    # JSONæ ¼å¼æ—¥å¿—
    class JsonFormatter(logging.Formatter):
        def format(self, record):
            log_entry = {
                "timestamp": self.formatTime(record),
                "level": record.levelname,
                "message": record.getMessage()
            }
            if hasattr(record, 'task_id'):
                log_entry["task_id"] = record.task_id
            return json.dumps(log_entry, ensure_ascii=False)
    
    formatter = JsonFormatter()
else:
    # æ–‡æœ¬æ ¼å¼æ—¥å¿—
    class TextFormatter(logging.Formatter):
        def format(self, record):
            log_message = super().format(record)
            if hasattr(record, 'task_id'):
                log_message = f"[task_{record.task_id}] {log_message}"
            return log_message
    
    formatter = TextFormatter("%(asctime)s [%(levelname)s] %(message)s")

# é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨
root_logger = logging.getLogger()
root_logger.setLevel(LOG_LEVEL)

# ç§»é™¤ç°æœ‰çš„å¤„ç†å™¨
for handler in root_logger.handlers[:]:
    root_logger.removeHandler(handler)
    handler.close()

# æ·»åŠ æ–°çš„å¤„ç†å™¨
# ä½¿ç”¨ RotatingFileHandler å®ç°æ—¥å¿—è½®è½¬
from logging.handlers import RotatingFileHandler
file_handler = RotatingFileHandler(
    LOG_FILE, 
    maxBytes=10*1024*1024,  # 10MB
    backupCount=20,
    encoding='utf-8'
)
file_handler.setFormatter(formatter)
console_handler = logging.StreamHandler()
console_handler.setFormatter(formatter)

root_logger.addHandler(file_handler)
root_logger.addHandler(console_handler)

logger = logging.getLogger(__name__)

# ä»»åŠ¡æ—¥å¿—å·¥å‚
def task_logger_factory(task_id):
    return LoggerAdapter(logger, {'task_id': task_id})

# å…è®¸å¤–éƒ¨è®¾ç½®ä»»åŠ¡æ—¥å¿—å·¥å‚çš„å‡½æ•°
_task_logger_factory = None

def set_task_logger_factory(factory):
    """è®¾ç½®å¤–éƒ¨ä»»åŠ¡æ—¥å¿—å·¥å‚"""
    global _task_logger_factory
    _task_logger_factory = factory

def get_task_logger(task_id):
    """è·å–ä»»åŠ¡æ—¥å¿—è®°å½•å™¨"""
    if _task_logger_factory:
        return _task_logger_factory(task_id)
    else:
        return task_logger_factory(task_id)

# åˆå§‹åŒ– Ollama å®¢æˆ·ç«¯ï¼Œä¼ é€’logger
ollama_client = create_ollama_client_from_config(config)


def clean_excel_string(s):
    """
    å½»åº•æ¸…æ´—å­—ç¬¦ä¸²ï¼Œç§»é™¤æ‰€æœ‰ Excel ä¸æ”¯æŒçš„æ§åˆ¶å­—ç¬¦å’Œå¸¸è§éšè—å­—ç¬¦ã€‚
    """
    if not isinstance(s, str):
        s = str(s)
    # ç§»é™¤ ASCII æ§åˆ¶å­—ç¬¦ï¼ˆä¿ç•™ \t \n \rï¼‰
    s = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', s)
    # ç§»é™¤ Unicode éšè—/æ ¼å¼å­—ç¬¦ï¼ˆé›¶å®½ç©ºæ ¼ã€BOMã€åŒå‘æ§åˆ¶ç¬¦ç­‰ï¼‰
    s = re.sub(r'[\u200B-\u200D\uFEFF\u202A-\u202E\u00AD\u180E]', '', s)
    return s.strip()


def is_valid_path(value, allow_filename_only=True):
    """
    ä½¿ç”¨Pythonå†…ç½®å‡½æ•°åˆ¤æ–­æ˜¯å¦ä¸ºåˆæ³•è·¯å¾„
    """
    if not isinstance(value, str):
        return False
    # æ‹’ç» URL
    if value.lower().startswith(('http://', 'https://', 'ftp://', 'file://', 'mailto:', 'javascript:')):
        return False
    # æ‹’ç»ç‰¹æ®Šæ ‡è®°
    if value.startswith('<') and value.endswith('>'):
        return False
    try:
        # ä½¿ç”¨Pathlibæ¥éªŒè¯è·¯å¾„
        path = Path(value)

        # æ£€æŸ¥è·¯å¾„æ˜¯å¦åŒ…å«éæ³•å­—ç¬¦ï¼ˆWindowsç‰¹å®šï¼‰
        if os.name == 'nt':  # Windowsç³»ç»Ÿ
            illegal_chars = '<>:"|?*'
            if any(char in value for char in illegal_chars):
                return False

        # å¦‚æœå…è®¸æ–‡ä»¶åä¸”ä¸æ˜¯ç»å¯¹è·¯å¾„ï¼Œåˆ™è®¤ä¸ºæ˜¯æœ‰æ•ˆçš„
        if allow_filename_only and not path.is_absolute():
            is_valid = len(value) <= 255
            # è®°å½•éªŒè¯ç»“æœ
            if not is_valid:
                logger.debug(f"è·¯å¾„éªŒè¯å¤±è´¥ï¼ˆæ–‡ä»¶åå¤ªé•¿ï¼‰: {repr(value)}")
            return is_valid

        # å¯¹äºç»å¯¹è·¯å¾„ï¼Œæ£€æŸ¥åŸºæœ¬æ ¼å¼
        if path.is_absolute():
            return True

        # å°è¯•è§„èŒƒåŒ–è·¯å¾„ï¼Œçœ‹æ˜¯å¦æœ‰æ•ˆ
        normalized = path.resolve()
        is_valid = str(normalized) != '/'
        # è®°å½•éªŒè¯ç»“æœ
        if not is_valid:
            logger.debug(f"è·¯å¾„éªŒè¯å¤±è´¥ï¼ˆè§„èŒƒåŒ–åæ— æ•ˆï¼‰: {repr(value)}")
        return is_valid

    except Exception as e:
        logger.debug(f"è·¯å¾„éªŒè¯å¼‚å¸¸: {repr(value)}, é”™è¯¯: {e}")
        return False


def clean_filter_string(filter_str):
    """
    æ¸…ç†è¿‡æ»¤æ¡ä»¶å­—ç¬¦ä¸²ï¼š
      - ç§»é™¤ 'ç»„ç»‡æœºæ„ = "..."'
      - ç§»é™¤ 'æ•°æ®æº = "..."'
    è¿”å›æ¸…ç†åçš„å­—ç¬¦ä¸²ï¼ˆä¿ç•™å…¶ä»–æ‰€æœ‰å†…å®¹ï¼ŒåŒ…æ‹¬ rlikeï¼‰
    """
    if not isinstance(filter_str, str):
        return filter_str

    cleaned = re.sub(r'\s*ç»„ç»‡æœºæ„\s*=\s*"[^"]*"', '', filter_str)
    cleaned = re.sub(r'\s*æ•°æ®æº\s*=\s*"[^"]*"', '', cleaned)
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    return cleaned


def call_ollama_model(input_text, task_id):
    task_logger = get_task_logger(task_id)
    task_logger.debug({"event": "ollama_input", "input": input_text})

    success, result_text, metadata = ollama_client.call_model(
        prompt=input_text,
        system_prompt=SYSTEM_PROMPT,
        temperature=0.0,
        num_predict=500,
        task_id=task_id
    )

    if not success:
        error_msg = metadata.get('error', 'Unknown error')
        task_logger.warning({"event": "ollama_call_failed", "error": error_msg})
        return [{
            "åºå·": int(task_id.split('_')[1]),
            "è¾“å…¥å†…å®¹": input_text,
            "åŸå§‹è·¯å¾„": clean_excel_string(f"<è°ƒç”¨å¤±è´¥: {error_msg}>"),
            "æ–‡ä»¶å": clean_excel_string("<æ— æ–‡ä»¶å>"),
            "ç±»å‹": "æœªçŸ¥",
            "åº”ç”¨åç§°": clean_excel_string("<æ— >")
        }]

    # --- ğŸ”§ æ–°å¢ï¼šå¢å¼º JSON æ¸…æ´—é€»è¾‘ ---
    cleaned_text = result_text.strip()
    
    # è®°å½•åŸå§‹å“åº”å’Œæ¸…æ´—å‰çš„æ–‡æœ¬
    task_logger.debug({"event": "raw_model_response", "response": result_text})
    
    # 1. ç§»é™¤å¼€å¤´çš„ "json" æˆ– "```json" ç­‰æ ‡è®°
    cleaned_text = re.sub(r'^```json\s*', '', cleaned_text)
    cleaned_text = re.sub(r'^```\s*json\s*', '', cleaned_text)
    cleaned_text = re.sub(r'^json\s*', '', cleaned_text, flags=re.IGNORECASE)
    
    # 2. ç§»é™¤ç»“å°¾çš„ "```"
    cleaned_text = re.sub(r'\s*```$', '', cleaned_text)
    
    # 3. æ‰¾åˆ° JSON å¼€å§‹ä½ç½®ï¼ˆç¬¬ä¸€ä¸ª '{' æˆ– '['ï¼‰
    start_brace = cleaned_text.find('{')
    start_bracket = cleaned_text.find('[')
    start_pos = min(start_brace if start_brace != -1 else float('inf'), start_bracket if start_bracket != -1 else float('inf'))
    if start_pos != float('inf'):
        cleaned_text = cleaned_text[int(start_pos):]
    else:
        # å¦‚æœæ‰¾ä¸åˆ°å¼€å§‹ç¬¦å·ï¼Œå°è¯•ä»ç¬¬ä¸€ä¸ªå­—æ¯å¼€å§‹æ‰¾å¯¹è±¡æˆ–æ•°ç»„
        cleaned_text = cleaned_text.lstrip()
    
    # 4. å°è¯•ä»åå¾€å‰æ‰¾åˆ°ç»“æŸç¬¦å·ï¼Œç¡®ä¿ JSON å®Œæ•´
    # ï¼ˆé˜²æ­¢ Ollama æˆªæ–­å“åº”ï¼‰
    last_brace = cleaned_text.rfind('}')
    last_bracket = cleaned_text.rfind(']')
    end_pos = max(last_brace, last_bracket)
    if end_pos != -1:
        cleaned_text = cleaned_text[:end_pos + 1]
    
    cleaned_text = cleaned_text.strip()
    
    # è®°å½•æ¸…æ´—åçš„æ–‡æœ¬
    task_logger.debug({"event": "cleaned_model_response", "response": cleaned_text})
    
    # --- END JSON æ¸…æ´— ---

    try:
        data = json.loads(cleaned_text)
    except json.JSONDecodeError as e:
        # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œè®°å½•æ›´è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
        task_logger.warning({
            "event": "json_parse_failed",
            "error": str(e),
            "cleaned_response": repr(cleaned_text),
            "original_response_snippet": result_text[:500]  # è®°å½•æ›´å¤šä¸Šä¸‹æ–‡
        })
        return [{
            "åºå·": int(task_id.split('_')[1]),
            "è¾“å…¥å†…å®¹": input_text,
            "åŸå§‹è·¯å¾„": clean_excel_string(f"<JSONè§£æå¤±è´¥: {str(e)[:100]}>"),
            "æ–‡ä»¶å": clean_excel_string("<æ— æ–‡ä»¶å>"),
            "ç±»å‹": "æœªçŸ¥",
            "åº”ç”¨åç§°": clean_excel_string("<æ— >")
        }]

    final_outputs = []
    if isinstance(data, list):
        for item in data:
            if isinstance(item, dict):
                path = clean_excel_string(item.get("path", "<æ— è·¯å¾„>"))
                filename = clean_excel_string(item.get("filename", "<æ— æ–‡ä»¶å>"))
                typ = clean_excel_string(item.get("type", "æœªçŸ¥"))
                app = clean_excel_string(item.get("app", "<æ— >"))
                
                # è®°å½•æ¯ä¸ªæå–çš„è·¯å¾„ä¿¡æ¯
                task_logger.debug({
                    "event": "extracted_path", 
                    "path": path, 
                    "filename": filename, 
                    "type": typ, 
                    "app": app
                })
                
                final_outputs.append({
                    "åºå·": int(task_id.split('_')[1]),
                    "è¾“å…¥å†…å®¹": input_text,
                    "åŸå§‹è·¯å¾„": path,
                    "æ–‡ä»¶å": filename,
                    "ç±»å‹": typ,
                    "åº”ç”¨åç§°": app
                })
    elif isinstance(data, dict):
        path = clean_excel_string(data.get("path", "<æ— è·¯å¾„>"))
        filename = clean_excel_string(data.get("filename", "<æ— æ–‡ä»¶å>"))
        typ = clean_excel_string(data.get("type", "æœªçŸ¥"))
        app = clean_excel_string(data.get("app", "<æ— >"))
        
        # è®°å½•æå–çš„è·¯å¾„ä¿¡æ¯
        task_logger.debug({
            "event": "extracted_path", 
            "path": path, 
            "filename": filename, 
            "type": typ, 
            "app": app
        })
        
        final_outputs.append({
            "åºå·": int(task_id.split('_')[1]),
            "è¾“å…¥å†…å®¹": input_text,
            "åŸå§‹è·¯å¾„": path,
            "æ–‡ä»¶å": filename,
            "ç±»å‹": typ,
            "åº”ç”¨åç§°": app
        })

    task_logger.debug({"event": "ollama_processed", "count": len(final_outputs)})
    return final_outputs


def process_row(row, idx):
    original_index = idx + 1
    row_dict = row.to_dict()

    input_text = ""

    # ä¼˜å…ˆä½¿ç”¨ "è¿‡æ»¤æ¡ä»¶" åˆ—ï¼ˆå¦‚æœå­˜åœ¨ä¸”éç©ºï¼‰
    if "è¿‡æ»¤æ¡ä»¶" in row_dict and pd.notna(row_dict["è¿‡æ»¤æ¡ä»¶"]):
        raw_filter = str(row_dict["è¿‡æ»¤æ¡ä»¶"]).strip()
        if raw_filter:
            input_text = clean_filter_string(raw_filter)
            logger.debug(f"æ¸…ç†åçš„è¿‡æ»¤æ¡ä»¶ (åºå·{original_index}): {repr(input_text)}")

    # å¦‚æœæ²¡æœ‰è¿‡æ»¤æ¡ä»¶ï¼Œæˆ–æ¸…ç†åä¸ºç©ºï¼Œåˆ™æ‹¼æ¥æ•´è¡Œï¼ˆè·³è¿‡ç»„ç»‡æœºæ„å’Œæ•°æ®æºï¼‰
    if not input_text.strip():
        parts = []
        # ä»é…ç½®ä¸­è¯»å–éœ€è¦å¿½ç•¥çš„åˆ—ï¼Œå¦‚æœé…ç½®ä¸ºç©ºåˆ™ä¸å¿½ç•¥ä»»ä½•åˆ—
        ignored_columns = config.get("processing", {}).get("ignored_columns", [])
        # å¦‚æœignored_columnsä¸ºNoneï¼Œå°†å…¶è®¾ç½®ä¸ºç©ºåˆ—è¡¨
        if ignored_columns is None:
            ignored_columns = []
        for col, val in row_dict.items():
            if col in ignored_columns:
                continue
            if pd.notna(val) and str(val).strip():
                parts.append(f"{col} = {str(val).strip()}")
        input_text = " ; ".join(parts)
        logger.debug(f"ä½¿ç”¨æ•´è¡Œæ‹¼æ¥ (åºå·{original_index}): {repr(input_text)}")

    # å¦‚æœæœ€ç»ˆ input_text ä»ä¸ºç©ºï¼Œåˆ™æ ‡è®°ä¸ºæ— è·¯å¾„
    if not input_text.strip():
        return {
            "type": "no_path_found",
            "row": row_dict
        }

    # ç›´æ¥å°†æ¸…ç†åçš„å†…å®¹äº¤ç»™ Ollama
    desc = "è¯·ä»ä»¥ä¸‹å®‰å…¨å‘Šè­¦å†…å®¹ä¸­æå–æ‰€æœ‰å¯ç–‘ç¨‹åºè·¯å¾„ã€æ–‡ä»¶åï¼Œå¹¶åˆ†ç±»è¾“å‡ºï¼š\n" + input_text
    parsed_results = call_ollama_model(desc, f"task_{original_index}")

    return {"type": "processed", "outputs": parsed_results}

